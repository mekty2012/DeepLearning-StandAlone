{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Attack With RNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "QjjUnbhZVNNN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attacking random with RNN"
      ]
    },
    {
      "metadata": {
        "id": "onmyxDkWVRPJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFfYvQCRVcUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing Data\n",
        "Input : 100 sequential random integer from java.util.Random.nextInt(1024)\n",
        "\n",
        "Output : next data right after such generation\n",
        "\n",
        "data set : 70000\n",
        "\n",
        "train : 50000\n",
        "\n",
        "validation : 10000"
      ]
    },
    {
      "metadata": {
        "id": "-e8wpSRMED2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "java.util.Random has following nextInt creation\n",
        "\n",
        "```\n",
        "def next(self, bits):\n",
        "    if bits < 1:\n",
        "        bits = 1\n",
        "    elif bits > 32:\n",
        "        bits = 32\n",
        "\n",
        "    self._seed = (self._seed * 0x5deece66d + 0xb) & ((1 << 48) - 1)\n",
        "    retval = self._seed >> (48 - bits)\n",
        "\n",
        "    return retval\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_9RolgMyVagp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b3e1040-3dc2-4ce3-e9c7-b2d25b148eae"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "_seed = \"aiming\".__hash__()\n",
        "\n",
        "def next(): # 32 bit integer\n",
        "    global _seed\n",
        "    _seed = (_seed * 0x5deece66d + 0xb) & ((1 << 48) - 1) \n",
        "    return _seed >> 38\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "for i in range(60000):\n",
        "    temp = []\n",
        "    for i in range(20):\n",
        "        ar = np.zeros(1024, dtype = np.float16)\n",
        "        ar[next()] = 1\n",
        "        temp.append(ar)\n",
        "    x.append(temp)\n",
        "    ar = np.zeros(1024, dtype = np.float16)\n",
        "    ar[next()] = 1\n",
        "    y.append(ar)\n",
        "\n",
        "npar_x = np.array(x)\n",
        "npar_y = np.array(y)\n",
        "torch_x = torch.from_numpy(npar_x)\n",
        "torch_y = torch.from_numpy(npar_y)\n",
        "print(npar_x.shape, npar_y.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 20, 1024) (60000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qc4wh-oRapzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_dataset = torch.utils.data.TensorDataset(torch_x,torch_y)\n",
        "trainset, valset = torch.utils.data.random_split(my_dataset, [50000, 10000])\n",
        "partition = {'train': trainset, 'val':valset}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7A--Qwkrj75u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Construction\n",
        "\n",
        "RNN model that accepts various models, we use MLP for each h2h, x2h, h2y."
      ]
    },
    {
      "metadata": {
        "id": "y9yKwcWvcvNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, batch_size, act, h2h, x2h, h2y):\n",
        "        \n",
        "        self.in_dim = 1024\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = 1024\n",
        "        self.batch_size = batch_size\n",
        "        self.act = act\n",
        "        \n",
        "        self.h2h = h2h\n",
        "        self.x2h = x2h\n",
        "        self.h2y = h2y\n",
        "        \n",
        "        if self.act == \"relu\": \n",
        "            self.act_fn = nn.ReLU()\n",
        "        elif self.act == \"tanh\": \n",
        "            self.act_fn = nn.Tanh()\n",
        "        elif self.act == \"sigmoid\": \n",
        "            self.act_fn = nn.sigmoid()\n",
        "        else: \n",
        "            raise ValueError(\"Illegal activation function\")\n",
        "        \n",
        "        self.hidden = self.init_hidden(self.batch_size)\n",
        "        \n",
        "    def init_hidden(self, batch_size = None):\n",
        "        if batch_size is None: batch_size = self.batch_size\n",
        "        return torch.zeros(batch_size, self.hid_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.act_fn(self.h2h(self.hidden) + self.x2h(x))\n",
        "        return self.h2y(h), h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yngaktOohDIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, use_bn, use_xavier, dropout):\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.use_bn = use_bn\n",
        "        self.use_xavier = use_xavier\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        \n",
        "        if self.act == \"relu\": \n",
        "            self.act_fn = nn.ReLU()\n",
        "        elif self.act == \"tanh\": \n",
        "            self.act_fn = nn.Tanh()\n",
        "        elif self.act == \"sigmoid\": \n",
        "            self.act_fn = nn.sigmoid()\n",
        "        else: \n",
        "            raise ValueError(\"Illegal activation function\")\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers.append(nn.Linear(in_dim, hid_dim))\n",
        "        self.layers.append(self.act_fn)\n",
        "        if self.use_bn:\n",
        "            self.layers.append(nn.BatchNorm1d(hid_dim))\n",
        "        self.layers.append(nn.Dropout(self.dropout))\n",
        "        \n",
        "        for i in range(n_layer-1):\n",
        "            self.layers.append(nn.Linear(hid_dim, hid_dim))\n",
        "            self.layers.append(self.act_fn)\n",
        "            if self.use_bn:\n",
        "                self.layers.append(nn.BatchNorm1d(hid_dim))\n",
        "            self.layers.append(nn.Dropout(self.dropout))\n",
        "        \n",
        "        self.layers.append(nn.Linear(hid_dim, out_dim))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MaSfaTIKUZh9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Train"
      ]
    },
    {
      "metadata": {
        "id": "OYOUcJG1UbTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "    net.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.view(-1, 3072)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v2_9soF1gTS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.valid_batch_size, \n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    net.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvBDvRR3gjdQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    x2h_net = MLP(args.in_dim, args.hidden_state_dim, args.x2h_hid_dim, args.x2h_n_layer, args.mlp_act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    h2h_net = MLP(args.hidden_state_dim, args.hidden_state_dim, args.h2h_hid_dim, args.h2h_n_layer, args.mlp_act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    h2y_net = MLP(args.hidden_state_dim, args.out_dim, args.h2y_hid_dim, args.h2y_n_layer, args.mlp_act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    net = RNN(args.train_batch_size, args.rnn_act, x2h_net, h2h_net, h2y_net)\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    return vars(args), result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_MxjU_A3wC3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Saving & Loading Result"
      ]
    },
    {
      "metadata": {
        "id": "GUWJ2FRi3vle",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBZkyUWu35UX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visulaizations"
      ]
    },
    {
      "metadata": {
        "id": "2iuRZ5Ra348m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_acc(var1, var2, df):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(15, 6)\n",
        "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
        "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
        "    \n",
        "    ax[0].set_title('Train Accuracy')\n",
        "    ax[1].set_title('Validation Accuracy')\n",
        "    \n",
        "    \n",
        "def plot_loss_variation(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
        "\n",
        "\n",
        "def plot_acc_variation(var1, var2, df, **kwargs):\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_accs = list(row.train_accs)[0]\n",
        "            val_accs = list(row.val_accs)[0]\n",
        "            test_acc = list(row.test_acc)[0]\n",
        "\n",
        "            for epoch, train_acc in enumerate(train_accs):\n",
        "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_acc in enumerate(val_accs):\n",
        "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Valid Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    \n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'val_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RlWaOniOT_CV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Argument Define"
      ]
    },
    {
      "metadata": {
        "id": "8U_256dpT-ld",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "seeds = [123, 456, 789]\n",
        "np.random.seed(seeds[0])\n",
        "torch.manual_seed(seeds[0])\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.exp_name = \n",
        "\n",
        "# ====== model related ====== #\n",
        "args.in_dim = 1024 #\n",
        "args.out_dim = 1024 #\n",
        "args.hidden_state_dim = \n",
        "\n",
        "args.hid_dim = \n",
        "args.n_layer = \n",
        "\n",
        "args.x2h_hid_dim = args.hid_dim\n",
        "args.x2h_n_layer = args.n_layer\n",
        "args.h2h_hid_dim = args.hid_dim\n",
        "args.h2h_n_layer = args.n_layer\n",
        "args.h2y_hid_dim = args.hid_dim\n",
        "args.h2y_n_layer = args.n_layer\n",
        "\n",
        "args.mlp_act = \n",
        "args.rnn_act = \"tanh\" #\n",
        "\n",
        "# ====== optimizer related ====== #\n",
        "args.lr = \n",
        "args.optim = \n",
        "args.epoch = 10 #\n",
        "\n",
        "# ====== regularization related ====== #\n",
        "args.dropout = 0.0\n",
        "args.l2 = 0.0\n",
        "args.use_bn = False\n",
        "args.use_xavier = False\n",
        "\n",
        "args.train_batch_size = \n",
        "args.test_batch_size = \n",
        "\n",
        "# ======  ====== #\n",
        "\n",
        "def grid_hyperparameter_tuning(default_namespace, var1, vals1, var2, vals2):\n",
        "    \n",
        "    namespace = deepcopy(default_namespace)\n",
        "    for val1 in vals1:\n",
        "        for val2 in vals2:\n",
        "            setattr(namespace, var1, val1)\n",
        "            setattr(namespace, var2, val2)\n",
        "            #print(namespace)\n",
        "                \n",
        "            setting, result = experiment(partition, deepcopy(namespace))\n",
        "            save_exp_result(setting, result)\n",
        "                \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
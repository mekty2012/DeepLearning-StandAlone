{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Attack With RNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QjjUnbhZVNNN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attacking random with RNN"
      ]
    },
    {
      "metadata": {
        "id": "onmyxDkWVRPJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFfYvQCRVcUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing Data\n",
        "Input : 100 sequential random integer from java.util.Random.nextInt(1024)\n",
        "\n",
        "Output : next data right after such generation\n",
        "\n",
        "data set : 70000\n",
        "\n",
        "train : 50000\n",
        "\n",
        "validation : 10000"
      ]
    },
    {
      "metadata": {
        "id": "-e8wpSRMED2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "java.util.Random has following nextInt creation\n",
        "\n",
        "```\n",
        "def next(self, bits):\n",
        "    if bits < 1:\n",
        "        bits = 1\n",
        "    elif bits > 32:\n",
        "        bits = 32\n",
        "\n",
        "    self._seed = (self._seed * 0x5deece66d + 0xb) & ((1 << 48) - 1)\n",
        "    retval = self._seed >> (48 - bits)\n",
        "\n",
        "    return retval\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_9RolgMyVagp",
        "colab_type": "code",
        "outputId": "dfd40059-0da4-48be-c597-8f7c5e0069e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "_seed = \"aiming\".__hash__()\n",
        "\n",
        "def next(): # 32 bit integer\n",
        "    global _seed\n",
        "    _seed = (_seed * 0x5deece66d + 0xb) & ((1 << 48) - 1) \n",
        "    return _seed >> 38\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "for i in range(60000):\n",
        "    temp = []\n",
        "    for i in range(20):\n",
        "        ar = np.zeros(1024, dtype = np.float32)\n",
        "        ar[next()] = 1\n",
        "        temp.append(ar)\n",
        "    x.append(temp)\n",
        "    ar = np.zeros(1024, dtype = np.float32)\n",
        "    ar[next()] = 1\n",
        "    y.append(ar)\n",
        "\n",
        "npar_x = np.array(x)\n",
        "npar_y = np.array(y)\n",
        "torch_x = torch.from_numpy(npar_x)\n",
        "torch_y = torch.from_numpy(npar_y)\n",
        "print(npar_x.shape, npar_y.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 20, 1024) (60000, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qc4wh-oRapzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_dataset = torch.utils.data.TensorDataset(torch_x, torch_y)\n",
        "trainset, valset = torch.utils.data.random_split(my_dataset, [50000, 10000])\n",
        "partition = {'train': trainset, 'val':valset}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7A--Qwkrj75u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Construction\n",
        "\n",
        "RNN model that accepts various models, we use MLP for each h2h, x2h, h2y."
      ]
    },
    {
      "metadata": {
        "id": "y9yKwcWvcvNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, batch_size, act, hidden_state_dim, h2h, x2h, h2y):\n",
        "        super(RNN, self).__init__()\n",
        "        self.in_dim = 1024\n",
        "        self.hidden_state_dim = hidden_state_dim\n",
        "        self.out_dim = 1024\n",
        "        self.batch_size = batch_size\n",
        "        self.act = act\n",
        "        \n",
        "        self.h2h = h2h\n",
        "        self.x2h = x2h\n",
        "        self.h2y = h2y\n",
        "        \n",
        "        if self.act == \"relu\": \n",
        "            self.act_fn = nn.ReLU()\n",
        "        elif self.act == \"tanh\": \n",
        "            self.act_fn = nn.Tanh()\n",
        "        elif self.act == \"sigmoid\": \n",
        "            self.act_fn = nn.sigmoid()\n",
        "        else: \n",
        "            raise ValueError(\"Illegal activation function\")\n",
        "        \n",
        "        self.hidden = self.init_hidden(self.batch_size)\n",
        "        \n",
        "    def init_hidden(self, batch_size = None):\n",
        "        if batch_size is None: batch_size = self.batch_size\n",
        "        return torch.zeros(batch_size, self.hidden_state_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for inp in x:\n",
        "            self.hidden = self.act_fn(self.h2h(self.hidden) + self.x2h(x))\n",
        "        return self.h2y(self.hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yngaktOohDIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, use_bn, use_xavier, dropout):\n",
        "        super(MLP, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.use_bn = use_bn\n",
        "        self.use_xavier = use_xavier\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        \n",
        "        if self.act == \"relu\": \n",
        "            self.act_fn = nn.ReLU()\n",
        "        elif self.act == \"tanh\": \n",
        "            self.act_fn = nn.Tanh()\n",
        "        elif self.act == \"sigmoid\": \n",
        "            self.act_fn = nn.sigmoid()\n",
        "        else: \n",
        "            raise ValueError(\"Illegal activation function\")\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers.append(nn.Linear(in_dim, hid_dim))\n",
        "        self.layers.append(self.act_fn)\n",
        "        if self.use_bn:\n",
        "            self.layers.append(nn.BatchNorm1d(hid_dim))\n",
        "        self.layers.append(nn.Dropout(self.dropout))\n",
        "        \n",
        "        for i in range(n_layer-1):\n",
        "            self.layers.append(nn.Linear(hid_dim, hid_dim))\n",
        "            self.layers.append(self.act_fn)\n",
        "            if self.use_bn:\n",
        "                self.layers.append(nn.BatchNorm1d(hid_dim))\n",
        "            self.layers.append(nn.Dropout(self.dropout))\n",
        "        \n",
        "        self.layers.append(nn.Linear(hid_dim, out_dim))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MaSfaTIKUZh9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Train"
      ]
    },
    {
      "metadata": {
        "id": "OYOUcJG1UbTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "    net.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        print(\"train inputs dim\", inputs.size())\n",
        "        #inputs = inputs.view(-1, args.train_batch_size, 1000)\n",
        "        if args.device == \"cuda\": \n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v2_9soF1gTS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.valid_batch_size, \n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    net.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            print(\"valid inputs dim\", inputs.size())\n",
        "            #images = images.view(-1, args.train_batch_size, 1000)\n",
        "            if args.device == \"cuda\": \n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvBDvRR3gjdQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    x2h_net = nn.Linear(args.in_dim, args.hidden_state_dim, bias = False) #MLP(args.in_dim, args.hidden_state_dim, args.x2h_hid_dim, args.x2h_n_layer, args.mlp_act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    h2h_net = nn.Linear(args.hidden_state_dim, args.hidden_state_dim, bias = False) #MLP(args.hidden_state_dim, args.hidden_state_dim, args.h2h_hid_dim, args.h2h_n_layer, args.mlp_act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    h2y_net = nn.Linear(args.hidden_state_dim, args.out_dim, bias = False)#MLP(args.hidden_state_dim, args.out_dim, args.h2y_hid_dim, args.h2y_n_layer, args.mlp_act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    net = RNN(args.train_batch_size, args.rnn_act, args.hidden_state_dim, x2h_net, h2h_net, h2y_net)\n",
        "    if args.device == \"cuda\": \n",
        "        net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    return vars(args), result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_MxjU_A3wC3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Saving & Loading Result"
      ]
    },
    {
      "metadata": {
        "id": "GUWJ2FRi3vle",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBZkyUWu35UX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visulaizations"
      ]
    },
    {
      "metadata": {
        "id": "2iuRZ5Ra348m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_acc(var1, var2, df):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(15, 6)\n",
        "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
        "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
        "    \n",
        "    ax[0].set_title('Train Accuracy')\n",
        "    ax[1].set_title('Validation Accuracy')\n",
        "    \n",
        "    \n",
        "def plot_loss_variation(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
        "\n",
        "\n",
        "def plot_acc_variation(var1, var2, df, **kwargs):\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_accs = list(row.train_accs)[0]\n",
        "            val_accs = list(row.val_accs)[0]\n",
        "            test_acc = list(row.test_acc)[0]\n",
        "\n",
        "            for epoch, train_acc in enumerate(train_accs):\n",
        "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_acc in enumerate(val_accs):\n",
        "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Valid Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    \n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'val_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RlWaOniOT_CV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Argument Define"
      ]
    },
    {
      "metadata": {
        "id": "8U_256dpT-ld",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.device = \"\"\n",
        "\n",
        "args.exp_name = \"exp1_hid_state_hid_dim\"\n",
        "\n",
        "# ====== model related ====== #\n",
        "args.in_dim = 1024 #\n",
        "args.out_dim = 1024 #\n",
        "args.hidden_state_dim = 128 # 32 64 128 256 512\n",
        "\n",
        "args.hid_dim = 128 # \n",
        "args.n_layer = 1\n",
        "\n",
        "args.x2h_hid_dim = args.hid_dim\n",
        "args.x2h_n_layer = args.n_layer\n",
        "args.h2h_hid_dim = args.hid_dim\n",
        "args.h2h_n_layer = args.n_layer\n",
        "args.h2y_hid_dim = args.hid_dim\n",
        "args.h2y_n_layer = args.n_layer\n",
        "\n",
        "args.mlp_act = \"relu\"\n",
        "args.rnn_act = \"tanh\" #\n",
        "\n",
        "# ====== optimizer related ====== #\n",
        "args.lr = .001\n",
        "args.optim = \"SGD\"\n",
        "args.epoch = 10 #\n",
        "\n",
        "# ====== regularization related ====== #\n",
        "args.dropout = 0.0\n",
        "args.l2 = 0.0\n",
        "args.weight_decay = args.l2\n",
        "args.use_bn = False\n",
        "args.use_xavier = False\n",
        "\n",
        "args.train_batch_size = 128\n",
        "args.test_batch_size = 128\n",
        "\n",
        "# ======  ====== #\n",
        "\n",
        "def grid_hyperparameter_tuning(default_namespace, var1, vals1, var2, vals2):\n",
        "    \n",
        "    namespace = deepcopy(default_namespace)\n",
        "    for val1 in vals1:\n",
        "        for val2 in vals2:\n",
        "            setattr(namespace, var1, val1)\n",
        "            setattr(namespace, var2, val2)\n",
        "            #print(namespace)\n",
        "                \n",
        "            setting, result = experiment(partition, deepcopy(namespace))\n",
        "            save_exp_result(setting, result)\n",
        "                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnG5GMD3dz-k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### hidden state dimension & hidden layer"
      ]
    },
    {
      "metadata": {
        "id": "wXfMByjIdzkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1209
        },
        "outputId": "758485ae-76c8-4f41-fcfd-44711af81883"
      },
      "cell_type": "code",
      "source": [
        "args.exp_name = \"exp1_hidden_state_dim__hid_dim\"\n",
        "grid_hyperparameter_tuning(args, \"hidden_state_dim\", [32, 64, 128, 256, 512], \"hid_dim\", [32, 64, 128, 256, 512])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train inputs dim torch.Size([128, 20, 1024])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-52424d434dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"exp1_hidden_state_dim__hid_dim\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrid_hyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden_state_dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hid_dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-a01b7d2a3f5a>\u001b[0m in \u001b[0;36mgrid_hyperparameter_tuning\u001b[0;34m(default_namespace, var1, vals1, var2, vals2)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m#print(namespace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0msave_exp_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-6fcf2ac56c0c>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(partition, args)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3fce8605850d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, partition, optimizer, criterion, args)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-136e02f8ca53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [128 x 32], m2: [1024 x 32] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:940"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "QsY0TmsmelKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "var1 = 'hidden_state_dim'\n",
        "var2 = 'hid_dim'\n",
        "df = load_exp_result(\"exp1_hidden_state_dim__hid_dim\")\n",
        "\n",
        "plot_acc(var1, var2, df)\n",
        "plot_loss_variation(var1, var2, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
        "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True) #margin_titles를 True로 하면 그래프의 가장자리에 var1과 var2 값이 표시되고 False로 하면 각 subplot 위에 표시됩니다."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}